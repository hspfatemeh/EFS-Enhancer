{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSkFt4sdEOtT",
        "outputId": "97fcd56d-3449-46fc-8482-fe39beb3712f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'iEnhancer-RD'...\n",
            "remote: Enumerating objects: 212, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 212 (delta 54), reused 0 (delta 0), pack-reused 112 (from 1)\u001b[K\n",
            "Receiving objects: 100% (212/212), 476.15 KiB | 4.81 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "Collecting Bio\n",
            "  Downloading bio-1.8.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from Bio) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from Bio) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from Bio) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from Bio) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython>=1.80->Bio) (2.0.2)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch->Bio) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pooch->Bio) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (2025.10.5)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from biothings-client>=0.2.6->mygene->Bio) (0.28.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.15.0)\n",
            "Downloading bio-1.8.1-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.8.1 biopython-1.86 biothings-client-0.4.1 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction**"
      ],
      "metadata": {
        "id": "dbuVmTqa5FiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5k8Mcoezp_m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from Bio.Seq import Seq\n",
        "from Bio import motifs\n",
        "from itertools import product\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import random\n",
        "import os\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "\n",
        "sequences = []\n",
        "with open(\"/dataset/B_Enhancer.txt\", \"r\") as file:\n",
        "    seq = \"\"\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\">\"):\n",
        "            if seq:\n",
        "                sequences.append(seq)\n",
        "                seq = \"\"\n",
        "        else:\n",
        "            seq += line\n",
        "    if seq:\n",
        "        sequences.append(seq)\n",
        "\n",
        "labels = [1 for i in range(len(sequences))]\n",
        "with open(\"/dataset/B_NonEnhancer.txt\", \"r\") as file:\n",
        "    seq = \"\"\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\">\"):\n",
        "            if seq:\n",
        "                sequences.append(seq)\n",
        "                seq = \"\"\n",
        "        else:\n",
        "            seq += line\n",
        "    if seq:\n",
        "        sequences.append(seq)\n",
        "labels = labels + [0 for i in range(len(sequences) - len(labels))]\n",
        "\n",
        "new_sequences = []\n",
        "with open(\"/dataset/I_Enhancer.txt\", \"r\") as file:\n",
        "    seq = \"\"\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\">\"):\n",
        "            if seq:\n",
        "                new_sequences.append(seq)\n",
        "                seq = \"\"\n",
        "        else:\n",
        "            seq += line\n",
        "    if seq:\n",
        "        new_sequences.append(seq)\n",
        "\n",
        "\n",
        "new_labels = [1 for i in range(len(new_sequences))]\n",
        "\n",
        "\n",
        "with open(\"/dataset/I_NonEnhancer.txt\", \"r\") as file:\n",
        "    seq = \"\"\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\">\"):\n",
        "            if seq:\n",
        "                new_sequences.append(seq)\n",
        "                seq = \"\"\n",
        "        else:\n",
        "            seq += line\n",
        "    if seq:\n",
        "        new_sequences.append(seq)\n",
        "\n",
        "new_labels = new_labels + [0 for i in range(len(new_sequences) - len(new_labels))]\n",
        "\n",
        "\n",
        "def load_dna2vec(path, k=3):\n",
        "    embedding_dict = {}\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            kmer = values[0]\n",
        "            if len(kmer) == k:\n",
        "                vector = np.array(values[1:], dtype=np.float32)\n",
        "                embedding_dict[kmer] = vector\n",
        "    return embedding_dict\n",
        "\n",
        "def kmer_tokenize_str(sequences, k=3):\n",
        "    kmer_seqs = []\n",
        "    for seq in sequences:\n",
        "        seq = seq.upper()\n",
        "        kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
        "        kmer_seqs.append(kmers)\n",
        "    return kmer_seqs\n",
        "\n",
        "def build_kmer_index(kmer_seqs):\n",
        "    kmer_set = set(k for seq in kmer_seqs for k in seq)\n",
        "    kmer_to_idx = {k: i+1 for i, k in enumerate(sorted(kmer_set))}\n",
        "    return kmer_to_idx\n",
        "\n",
        "def encode_kmers(kmer_seqs, kmer_to_idx):\n",
        "    encoded = np.zeros((len(kmer_seqs), len(kmer_seqs[0])), dtype=np.int32)\n",
        "    for i, seq in enumerate(kmer_seqs):\n",
        "        for j, kmer in enumerate(seq):\n",
        "            encoded[i, j] = kmer_to_idx.get(kmer, 0)\n",
        "    return encoded\n",
        "\n",
        "def build_embedding_matrix(kmer_to_idx, embedding_dict, embedding_dim):\n",
        "    vocab_size = len(kmer_to_idx) + 1\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for kmer, idx in kmer_to_idx.items():\n",
        "        if kmer in embedding_dict:\n",
        "            embedding_matrix[idx] = embedding_dict[kmer]\n",
        "        else:\n",
        "            embedding_matrix[idx] = np.random.normal(scale=0.1, size=(embedding_dim,))\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "k = 4\n",
        "embedding_dim = 100\n",
        "dna2vec_path = '/content/dna2vec-20251027-1738-k3to5-100d-2c-0Mbp-sliding-338.w2v'\n",
        "kmer_seqs = kmer_tokenize_str(sequences, k=k)\n",
        "kmer_to_idx = build_kmer_index(kmer_seqs)\n",
        "embedding_dict = load_dna2vec(dna2vec_path, k=k)\n",
        "embedding_matrix = build_embedding_matrix(kmer_to_idx, embedding_dict, embedding_dim)\n",
        "encoded_seqs = encode_kmers(kmer_seqs, kmer_to_idx)\n",
        "kmer_seqs_new = kmer_tokenize_str(new_sequences, k=k)\n",
        "encoded_seqs_new = encode_kmers(kmer_seqs_new, kmer_to_idx)\n",
        "\n",
        "\n",
        "def seqs_to_embedded_vectors(encoded_seqs, embedding_matrix):\n",
        "    vectors = []\n",
        "    for seq in encoded_seqs:\n",
        "        emb = embedding_matrix[seq]\n",
        "        avg_emb = np.mean(emb, axis=0)\n",
        "        vectors.append(avg_emb)\n",
        "    return np.array(vectors)\n",
        "\n",
        "X_train = seqs_to_embedded_vectors(encoded_seqs, embedding_matrix)\n",
        "X_test = seqs_to_embedded_vectors(encoded_seqs_new, embedding_matrix)\n",
        "\n",
        "\n",
        "def performance(labelArr, predictArr):\n",
        "    TN, FP, FN, TP = metrics.confusion_matrix(labelArr, predictArr).ravel()\n",
        "    ACC = metrics.accuracy_score(labelArr, predictArr)\n",
        "    SN = metrics.recall_score(labelArr, predictArr)\n",
        "    SP = TN/(FP + TN)\n",
        "    MCC= matthews_corrcoef(labelArr, predictArr)\n",
        "    return ACC,SN,SP,MCC\n",
        "\n",
        "def compute_gc_content(seq):\n",
        "    seq = seq.upper()\n",
        "    gc_count = seq.count('G') + seq.count('C')\n",
        "    return gc_count / len(seq) if len(seq) > 0 else 0\n",
        "\n",
        "def compute_sequence_entropy(seq, k=3):\n",
        "    seq = seq.upper()\n",
        "    kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
        "    kmer_counts = Counter(kmers)\n",
        "    probs = [count / len(kmers) for count in kmer_counts.values()]\n",
        "    return entropy(probs)\n",
        "\n",
        "def extract_stat_features(seqs, k=3):\n",
        "    features = []\n",
        "    for seq in seqs:\n",
        "        gc = compute_gc_content(seq)\n",
        "        ent = compute_sequence_entropy(seq, k=k)\n",
        "        features.append([gc, ent])\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def make_pwm_from_consensus(consensus):\n",
        "    instances = [Seq(consensus)]\n",
        "    m = motifs.create(instances)\n",
        "    pwm = m.counts.normalize(pseudocounts=0.1)\n",
        "    return pwm\n",
        "\n",
        "def pwm_score_features(sequences, motif_list):\n",
        "    pwm_logodds_list = []\n",
        "    for motif in motif_list:\n",
        "        pwm = make_pwm_from_consensus(motif)\n",
        "        log_odds = pwm.log_odds()\n",
        "        pwm_logodds_list.append(log_odds)\n",
        "    features = []\n",
        "    for seq in sequences:\n",
        "        seq = Seq(seq.upper())\n",
        "        seq_scores = []\n",
        "        for log_odds in pwm_logodds_list:\n",
        "            scores = [score for _, score in log_odds.search(seq)]\n",
        "            max_score = max(scores) if scores else 0.0\n",
        "            seq_scores.append(max_score)\n",
        "        features.append(seq_scores)\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def get_all_kmers(k):\n",
        "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
        "\n",
        "def kmer_frequency_features(sequences, k=3):\n",
        "    all_kmers = get_all_kmers(k)\n",
        "    features = []\n",
        "    for seq in sequences:\n",
        "        seq = seq.upper()\n",
        "        kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
        "        kmer_counts = Counter(kmers)\n",
        "        total = sum(kmer_counts.values())\n",
        "        freq_vector = [kmer_counts[kmer] / total if total > 0 else 0.0 for kmer in all_kmers]\n",
        "        features.append(freq_vector)\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "def dinuc_freq(sequences):\n",
        "    dinucs = [a+b for a in 'ACGT' for b in 'ACGT']\n",
        "    features = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        seq = seq.upper()\n",
        "        total = len(seq) - 1\n",
        "        counts = Counter([seq[i:i+2] for i in range(total)])\n",
        "        freq = [counts[d]/total if total > 0 else 0 for d in dinucs]\n",
        "        features.append(freq)\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "X_train_dinuc = dinuc_freq(sequences)\n",
        "X_test_dinuc = dinuc_freq(new_sequences)\n",
        "motifs_list = ['TATA','CCCTG','CCTGG','GCCTG','CCCAGG','CCCAGCC','TTGGGAG']\n",
        "motif_features_train = pwm_score_features(sequences, motifs_list)\n",
        "motif_features_test = pwm_score_features(new_sequences, motifs_list)\n",
        "stat_features_train = extract_stat_features(sequences, k=3)\n",
        "stat_features_test = extract_stat_features(new_sequences, k=3)\n",
        "kmer_freq_train = kmer_frequency_features(sequences, k=3)\n",
        "kmer_freq_test = kmer_frequency_features(new_sequences, k=3)\n",
        "\n",
        "X_train = np.concatenate([X_train, kmer_freq_train], axis=1)\n",
        "X_test = np.concatenate([X_test, kmer_freq_test], axis=1)\n",
        "X_train = np.concatenate([X_train, stat_features_train], axis=1)\n",
        "X_test = np.concatenate([X_test, stat_features_test], axis=1)\n",
        "X_train = np.concatenate([X_train, motif_features_train], axis=1)\n",
        "X_test = np.concatenate([X_test, motif_features_test], axis=1)\n",
        "X_train = np.concatenate([X_train, X_train_dinuc], axis=1)\n",
        "X_test = np.concatenate([X_test, X_test_dinuc], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN**"
      ],
      "metadata": {
        "id": "VXzTi4005CIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnCwb1DT1gOb",
        "outputId": "12ece937-5d2d-4be2-ad9c-736bedecb93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.6546 - loss: 0.7548 - val_accuracy: 0.0017 - val_loss: 0.7849\n",
            "Epoch 2/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7283 - loss: 0.5436 - val_accuracy: 0.0000e+00 - val_loss: 0.9538\n",
            "Epoch 3/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7258 - loss: 0.5307 - val_accuracy: 0.0000e+00 - val_loss: 1.0170\n",
            "Epoch 4/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.7269 - loss: 0.5298 - val_accuracy: 0.0118 - val_loss: 1.0221\n",
            "Epoch 5/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7226 - loss: 0.5441 - val_accuracy: 0.0269 - val_loss: 1.0912\n",
            "Epoch 6/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7560 - loss: 0.5021 - val_accuracy: 0.2290 - val_loss: 0.8872\n",
            "Epoch 7/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7584 - loss: 0.5081 - val_accuracy: 0.6027 - val_loss: 0.7210\n",
            "Epoch 8/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.7563 - loss: 0.5032 - val_accuracy: 0.5892 - val_loss: 0.7617\n",
            "Epoch 9/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.7417 - loss: 0.5098 - val_accuracy: 0.5774 - val_loss: 0.7698\n",
            "Epoch 10/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7577 - loss: 0.4975 - val_accuracy: 0.6077 - val_loss: 0.7994\n",
            "Epoch 11/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7553 - loss: 0.5042 - val_accuracy: 0.6498 - val_loss: 0.6802\n",
            "Epoch 12/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.7414 - loss: 0.5120 - val_accuracy: 0.6751 - val_loss: 0.6723\n",
            "Epoch 13/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.7663 - loss: 0.4947 - val_accuracy: 0.7492 - val_loss: 0.5878\n",
            "Epoch 14/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7312 - loss: 0.5122 - val_accuracy: 0.5539 - val_loss: 0.8873\n",
            "Epoch 15/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7501 - loss: 0.5148 - val_accuracy: 0.6650 - val_loss: 0.6886\n",
            "Epoch 16/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.7631 - loss: 0.4824 - val_accuracy: 0.6296 - val_loss: 0.7032\n",
            "Epoch 17/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7687 - loss: 0.4871 - val_accuracy: 0.6465 - val_loss: 0.7575\n",
            "Epoch 18/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.7628 - loss: 0.4879 - val_accuracy: 0.6178 - val_loss: 0.7736\n",
            "Epoch 19/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7502 - loss: 0.5021 - val_accuracy: 0.5690 - val_loss: 0.7814\n",
            "Epoch 20/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7666 - loss: 0.4874 - val_accuracy: 0.6599 - val_loss: 0.6977\n",
            "Epoch 21/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.7501 - loss: 0.5089 - val_accuracy: 0.6397 - val_loss: 0.7748\n",
            "Epoch 22/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7671 - loss: 0.4852 - val_accuracy: 0.7306 - val_loss: 0.5839\n",
            "Epoch 23/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7519 - loss: 0.5162 - val_accuracy: 0.6852 - val_loss: 0.6628\n",
            "Epoch 24/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.7673 - loss: 0.4881 - val_accuracy: 0.6448 - val_loss: 0.7685\n",
            "Epoch 25/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.7492 - loss: 0.4934 - val_accuracy: 0.7492 - val_loss: 0.6042\n",
            "Epoch 26/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.7551 - loss: 0.4986 - val_accuracy: 0.6633 - val_loss: 0.6666\n",
            "Epoch 27/30\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.7724 - loss: 0.4761 - val_accuracy: 0.7037 - val_loss: 0.6788\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\n",
            "Test Set Results:\n",
            "Accuracy: 0.7725\n",
            "Sensitivity (Recall): 0.8100\n",
            "Specificity: 0.7350\n",
            "MCC: 0.5465\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(labels)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(new_labels)\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv1D(128, 3, activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(2),\n",
        "        Dropout(0.3),\n",
        "        Conv1D(32, 3, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(2),\n",
        "        Dropout(0.3),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.4),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_cnn_model((X_train.shape[1], 1))\n",
        "\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train, validation_split=0.2, epochs=30,\n",
        "    batch_size=32, callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "ACC = accuracy_score(y_test, y_pred_test)\n",
        "SN = recall_score(y_test, y_pred_test)\n",
        "SP = TN / (TN + FP)\n",
        "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\nTest Set Results:\")\n",
        "print(f\"Accuracy: {ACC:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {SN:.4f}\")\n",
        "print(f\"Specificity: {SP:.4f}\")\n",
        "print(f\"MCC: {MCC:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWqrZiL-7u_V"
      },
      "source": [
        "**CNN-Multi-Head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IvJenpCR74gU",
        "outputId": "efa14ef6-4dd0-4d19-da78-f284148222fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_16\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_16\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_16      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_32 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m2,048\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ conv1d_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_32    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_60          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_32… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_33 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │     \u001b[38;5;34m40,992\u001b[0m │ dropout_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │        \u001b[38;5;34m128\u001b[0m │ conv1d_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_33    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_61          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_33… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │     \u001b[38;5;34m92,256\u001b[0m │ dropout_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m1,056\u001b[0m │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_12 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dropout_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │                   │            │ dense_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │         \u001b[38;5;34m64\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │      \u001b[38;5;34m8,448\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_63          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m8,224\u001b[0m │ dropout_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_13 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│                     │                   │            │ dense_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │         \u001b[38;5;34m64\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m4,224\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_64          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_16      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_32    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_60          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_32… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">40,992</span> │ dropout_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv1d_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_33    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_61          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_33… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">92,256</span> │ dropout_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │                   │            │ dense_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,448</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_63          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │ dropout_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│                     │                   │            │ dense_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_64          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m158,657\u001b[0m (619.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,657</span> (619.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m158,081\u001b[0m (617.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,081</span> (617.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m576\u001b[0m (2.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> (2.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 400ms/step - accuracy: 0.6220 - loss: 0.6648 - val_accuracy: 1.0000 - val_loss: 0.3444 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 410ms/step - accuracy: 0.6492 - loss: 0.6317 - val_accuracy: 1.0000 - val_loss: 0.1847 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 383ms/step - accuracy: 0.6636 - loss: 0.6127 - val_accuracy: 1.0000 - val_loss: 0.1705 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 388ms/step - accuracy: 0.7229 - loss: 0.5653 - val_accuracy: 1.0000 - val_loss: 0.1513 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 361ms/step - accuracy: 0.7196 - loss: 0.5629 - val_accuracy: 1.0000 - val_loss: 0.1343 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 374ms/step - accuracy: 0.7301 - loss: 0.5423 - val_accuracy: 1.0000 - val_loss: 0.1186 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 361ms/step - accuracy: 0.7589 - loss: 0.5132 - val_accuracy: 1.0000 - val_loss: 0.1131 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 373ms/step - accuracy: 0.7395 - loss: 0.5220 - val_accuracy: 1.0000 - val_loss: 0.1000 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 395ms/step - accuracy: 0.7515 - loss: 0.5091 - val_accuracy: 1.0000 - val_loss: 0.1067 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 388ms/step - accuracy: 0.7568 - loss: 0.5110 - val_accuracy: 1.0000 - val_loss: 0.1223 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 417ms/step - accuracy: 0.7571 - loss: 0.5103 - val_accuracy: 1.0000 - val_loss: 0.1496 - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 385ms/step - accuracy: 0.7584 - loss: 0.5051 - val_accuracy: 0.9815 - val_loss: 0.1842 - learning_rate: 1.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 385ms/step - accuracy: 0.7469 - loss: 0.5249 - val_accuracy: 0.9360 - val_loss: 0.2488 - learning_rate: 1.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 394ms/step - accuracy: 0.7612 - loss: 0.5073 - val_accuracy: 0.9175 - val_loss: 0.2881 - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 384ms/step - accuracy: 0.7572 - loss: 0.5079 - val_accuracy: 0.8906 - val_loss: 0.3562 - learning_rate: 1.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 0.7390 - loss: 0.5117\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 395ms/step - accuracy: 0.7396 - loss: 0.5113 - val_accuracy: 0.8771 - val_loss: 0.3671 - learning_rate: 1.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 402ms/step - accuracy: 0.7509 - loss: 0.5174 - val_accuracy: 0.8418 - val_loss: 0.4438 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 382ms/step - accuracy: 0.7470 - loss: 0.5155 - val_accuracy: 0.8030 - val_loss: 0.5080 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 373ms/step - accuracy: 0.7566 - loss: 0.4964\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 399ms/step - accuracy: 0.7566 - loss: 0.4966 - val_accuracy: 0.8081 - val_loss: 0.4994 - learning_rate: 5.0000e-05\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step\n",
            "\n",
            " Test Set Results:\n",
            "Accuracy: 0.6600\n",
            "Sensitivity (SN): 0.3750\n",
            "Specificity (SP): 0.9450\n",
            "MCC: 0.3895\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization,\n",
        "    MultiHeadAttention, Add, LayerNormalization, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(labels)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(new_labels)\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "def build_advanced_cnn_transformer(input_shape,\n",
        "                                   num_heads=8,\n",
        "                                   key_dim=64,\n",
        "                                   dropout_rate=0.4,\n",
        "                                   dense_units=256,\n",
        "                                   lr=1e-4):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv1D(256, 7, activation='relu', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Conv1D(32, 5, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
        "    attn_output = Dense(x.shape[-1])(attn_output)\n",
        "    x = Add()([x, attn_output])\n",
        "    x = LayerNormalization()(x)\n",
        "    ffn = Dense(dense_units, activation='relu')(x)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "    ffn = Dense(x.shape[-1])(ffn)\n",
        "    x = Add()([x, ffn])\n",
        "    x = LayerNormalization()(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_advanced_cnn_transformer(\n",
        "    input_shape=(X_train.shape[1], 1),\n",
        "    num_heads=11,\n",
        "    key_dim=64,\n",
        "    dropout_rate=0.3,\n",
        "    dense_units=256,\n",
        "    lr=1e-4\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='accuracy', patience=6, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=3, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50, batch_size=64, callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "ACC = accuracy_score(y_test, y_pred_test)\n",
        "SN = recall_score(y_test, y_pred_test)\n",
        "SP = TN / (TN + FP)\n",
        "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\n Test Set Results:\")\n",
        "print(f\"Accuracy: {ACC:.4f}\")\n",
        "print(f\"Sensitivity (SN): {SN:.4f}\")\n",
        "print(f\"Specificity (SP): {SP:.4f}\")\n",
        "print(f\"MCC: {MCC:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJnW1mHpM7ry"
      },
      "source": [
        "**CNN-BiLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2X3adyMNBep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8f12f9-bc3a-4078-c3dd-78d66fe7829e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 190ms/step - accuracy: 0.5646 - loss: 0.6815 - val_accuracy: 1.0000 - val_loss: 0.6523 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.6166 - loss: 0.6626 - val_accuracy: 1.0000 - val_loss: 0.6134 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - accuracy: 0.6253 - loss: 0.6381 - val_accuracy: 1.0000 - val_loss: 0.5858 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 162ms/step - accuracy: 0.6462 - loss: 0.6120 - val_accuracy: 1.0000 - val_loss: 0.5662 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - accuracy: 0.7062 - loss: 0.5793 - val_accuracy: 1.0000 - val_loss: 0.5521 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 193ms/step - accuracy: 0.7093 - loss: 0.5574 - val_accuracy: 1.0000 - val_loss: 0.5297 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 164ms/step - accuracy: 0.7425 - loss: 0.5287 - val_accuracy: 1.0000 - val_loss: 0.5002 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 164ms/step - accuracy: 0.7560 - loss: 0.5160 - val_accuracy: 1.0000 - val_loss: 0.4850 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 162ms/step - accuracy: 0.7467 - loss: 0.5088 - val_accuracy: 1.0000 - val_loss: 0.4776 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 194ms/step - accuracy: 0.7465 - loss: 0.5214 - val_accuracy: 0.9663 - val_loss: 0.4676 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - accuracy: 0.7684 - loss: 0.4940 - val_accuracy: 0.9242 - val_loss: 0.4625 - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 193ms/step - accuracy: 0.7561 - loss: 0.5043 - val_accuracy: 0.8704 - val_loss: 0.4919 - learning_rate: 1.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 199ms/step - accuracy: 0.7417 - loss: 0.5232 - val_accuracy: 0.8316 - val_loss: 0.5135 - learning_rate: 1.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 165ms/step - accuracy: 0.7716 - loss: 0.4942 - val_accuracy: 0.7441 - val_loss: 0.5938 - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 163ms/step - accuracy: 0.7339 - loss: 0.5215 - val_accuracy: 0.7694 - val_loss: 0.5691 - learning_rate: 1.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7504 - loss: 0.5073\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 194ms/step - accuracy: 0.7503 - loss: 0.5074 - val_accuracy: 0.7559 - val_loss: 0.5898 - learning_rate: 1.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - accuracy: 0.7610 - loss: 0.5023 - val_accuracy: 0.7205 - val_loss: 0.6589 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 194ms/step - accuracy: 0.7583 - loss: 0.4946 - val_accuracy: 0.7256 - val_loss: 0.6498 - learning_rate: 5.0000e-05\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step\n",
            "\n",
            " Test Set Results:\n",
            "Accuracy: 0.7250\n",
            "Sensitivity (SN): 0.6500\n",
            "Specificity (SP): 0.8000\n",
            "MCC: 0.4551\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv1D, MaxPooling1D, Flatten, Dense, Dropout,\n",
        "    BatchNormalization, LSTM, Bidirectional\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(labels)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(new_labels)\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "def build_cnn_lstm_model(input_shape, lstm_units=128, dense_units=128,\n",
        "                         dropout_rate=0.4, lr=1e-4):\n",
        "    model = Sequential([\n",
        "        Conv1D(128, 5, activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(2),\n",
        "        Dropout(0.3),\n",
        "        Conv1D(64, 3, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(2),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(lstm_units, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)),\n",
        "        Dense(dense_units, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_cnn_lstm_model(\n",
        "    input_shape=(X_train.shape[1], 1), lstm_units=32,\n",
        "    dense_units=128,\n",
        "    dropout_rate=0.3, lr=1e-4\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='accuracy', patience=5, restore_best_weights=True, verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='accuracy', factor=0.5, patience=3, verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "ACC = accuracy_score(y_test, y_pred_test)\n",
        "SN = recall_score(y_test, y_pred_test)\n",
        "SP = TN / (TN + FP)\n",
        "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\n Test Set Results:\")\n",
        "print(f\"Accuracy: {ACC:.4f}\")\n",
        "print(f\"Sensitivity (SN): {SN:.4f}\")\n",
        "print(f\"Specificity (SP): {SP:.4f}\")\n",
        "print(f\"MCC: {MCC:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZyv4280gDUP"
      },
      "source": [
        "**BiLSTM‌**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FntKVrpJM7Sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed041675-5008-4b29-87e8-7d2b9097c400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 455ms/step - accuracy: 0.6268 - loss: 0.7058 - val_accuracy: 0.1902 - val_loss: 0.6965 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 416ms/step - accuracy: 0.6857 - loss: 0.5816 - val_accuracy: 0.0000e+00 - val_loss: 0.7362 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 442ms/step - accuracy: 0.7046 - loss: 0.5734 - val_accuracy: 0.0000e+00 - val_loss: 0.8174 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 419ms/step - accuracy: 0.7301 - loss: 0.5440 - val_accuracy: 0.0000e+00 - val_loss: 0.9049 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 445ms/step - accuracy: 0.7287 - loss: 0.5277 - val_accuracy: 0.0000e+00 - val_loss: 0.8761 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 422ms/step - accuracy: 0.7403 - loss: 0.5316 - val_accuracy: 0.0000e+00 - val_loss: 0.8886 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 418ms/step - accuracy: 0.7311 - loss: 0.5259 - val_accuracy: 0.0000e+00 - val_loss: 0.8516 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 424ms/step - accuracy: 0.7413 - loss: 0.5277 - val_accuracy: 0.0000e+00 - val_loss: 0.8434 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 409ms/step - accuracy: 0.7430 - loss: 0.5285 - val_accuracy: 0.0000e+00 - val_loss: 0.8871 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 437ms/step - accuracy: 0.7429 - loss: 0.5048 - val_accuracy: 0.0875 - val_loss: 0.7871 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 413ms/step - accuracy: 0.7297 - loss: 0.5196 - val_accuracy: 0.5387 - val_loss: 0.6989 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - accuracy: 0.7386 - loss: 0.5047\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 424ms/step - accuracy: 0.7385 - loss: 0.5050 - val_accuracy: 0.2475 - val_loss: 0.7862 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 420ms/step - accuracy: 0.7448 - loss: 0.5267 - val_accuracy: 0.7778 - val_loss: 0.6046 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 438ms/step - accuracy: 0.7464 - loss: 0.4969 - val_accuracy: 0.9394 - val_loss: 0.4698 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 418ms/step - accuracy: 0.7470 - loss: 0.4953 - val_accuracy: 0.8114 - val_loss: 0.5278 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 408ms/step - accuracy: 0.7435 - loss: 0.5119 - val_accuracy: 0.7946 - val_loss: 0.5620 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 417ms/step - accuracy: 0.7311 - loss: 0.5218 - val_accuracy: 0.7576 - val_loss: 0.5779 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 409ms/step - accuracy: 0.7605 - loss: 0.4997 - val_accuracy: 0.5960 - val_loss: 0.8087 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 416ms/step - accuracy: 0.7468 - loss: 0.4973 - val_accuracy: 0.3838 - val_loss: 1.1865 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 440ms/step - accuracy: 0.7368 - loss: 0.5060 - val_accuracy: 0.3114 - val_loss: 1.3724 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 408ms/step - accuracy: 0.7622 - loss: 0.4896 - val_accuracy: 0.3586 - val_loss: 1.3387 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 407ms/step - accuracy: 0.7582 - loss: 0.4937 - val_accuracy: 0.1801 - val_loss: 1.7438 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 420ms/step - accuracy: 0.7610 - loss: 0.4964 - val_accuracy: 0.4630 - val_loss: 1.1271 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 428ms/step - accuracy: 0.7534 - loss: 0.4975 - val_accuracy: 0.6818 - val_loss: 0.7457 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 414ms/step - accuracy: 0.7439 - loss: 0.4964 - val_accuracy: 0.4343 - val_loss: 1.0022 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.7596 - loss: 0.4822\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 425ms/step - accuracy: 0.7594 - loss: 0.4825 - val_accuracy: 0.8653 - val_loss: 0.3746 - learning_rate: 5.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 413ms/step - accuracy: 0.7635 - loss: 0.4972 - val_accuracy: 0.8636 - val_loss: 0.3607 - learning_rate: 2.5000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 408ms/step - accuracy: 0.7545 - loss: 0.4908 - val_accuracy: 0.8165 - val_loss: 0.4595 - learning_rate: 2.5000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 407ms/step - accuracy: 0.7583 - loss: 0.4863 - val_accuracy: 0.6313 - val_loss: 0.7620 - learning_rate: 2.5000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.7649 - loss: 0.4929\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 440ms/step - accuracy: 0.7647 - loss: 0.4929 - val_accuracy: 0.6330 - val_loss: 0.8191 - learning_rate: 2.5000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 419ms/step - accuracy: 0.7698 - loss: 0.4817 - val_accuracy: 0.5993 - val_loss: 0.8146 - learning_rate: 1.2500e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 409ms/step - accuracy: 0.7586 - loss: 0.4930 - val_accuracy: 0.7037 - val_loss: 0.7056 - learning_rate: 1.2500e-04\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step\n",
            "\n",
            " Test Set Results:\n",
            "Accuracy: 0.7225\n",
            "Sensitivity (Recall): 0.6100\n",
            "Specificity: 0.8350\n",
            "MCC: 0.4567\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(labels)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(new_labels)\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "def build_bilstm_model(input_shape, lr=0.001):\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(32, return_sequences=True), input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(LSTM(64)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_bilstm_model((X_train.shape[1], 1))\n",
        "\n",
        "early_stop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=3, min_lr=1e-4, verbose=1)\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "ACC = accuracy_score(y_test, y_pred_test)\n",
        "SN = recall_score(y_test, y_pred_test)\n",
        "SP = TN / (TN + FP)\n",
        "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\n Test Set Results:\")\n",
        "print(f\"Accuracy: {ACC:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {SN:.4f}\")\n",
        "print(f\"Specificity: {SP:.4f}\")\n",
        "print(f\"MCC: {MCC:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFIGrdSaAjZv"
      },
      "source": [
        "**CNN-BiLSTM-Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujDcse5M7It",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a8b2bb-ceac-4f1e-86d0-b124082a5886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 198ms/step - accuracy: 0.5434 - loss: 0.6887 - val_accuracy: 0.0000e+00 - val_loss: 0.7098 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 193ms/step - accuracy: 0.6244 - loss: 0.6712 - val_accuracy: 0.0000e+00 - val_loss: 0.7218 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 179ms/step - accuracy: 0.6200 - loss: 0.6635 - val_accuracy: 0.0000e+00 - val_loss: 0.7222 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 179ms/step - accuracy: 0.6093 - loss: 0.6637 - val_accuracy: 0.0000e+00 - val_loss: 0.7175 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 195ms/step - accuracy: 0.6333 - loss: 0.6471 - val_accuracy: 0.0000e+00 - val_loss: 0.7101 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.6263 - loss: 0.6455\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - accuracy: 0.6263 - loss: 0.6454 - val_accuracy: 0.0034 - val_loss: 0.7067 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 195ms/step - accuracy: 0.6271 - loss: 0.6362 - val_accuracy: 0.0707 - val_loss: 0.7037 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - accuracy: 0.6319 - loss: 0.6304 - val_accuracy: 0.0774 - val_loss: 0.7085 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 196ms/step - accuracy: 0.6315 - loss: 0.6224 - val_accuracy: 0.0354 - val_loss: 0.7225 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - accuracy: 0.6384 - loss: 0.6167 - val_accuracy: 0.0522 - val_loss: 0.7309 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 196ms/step - accuracy: 0.6508 - loss: 0.6077 - val_accuracy: 0.1162 - val_loss: 0.7358 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 184ms/step - accuracy: 0.6577 - loss: 0.5993 - val_accuracy: 0.1229 - val_loss: 0.7576 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - accuracy: 0.6651 - loss: 0.5943 - val_accuracy: 0.2828 - val_loss: 0.7521 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 191ms/step - accuracy: 0.7236 - loss: 0.5703 - val_accuracy: 0.4192 - val_loss: 0.7442 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 160ms/step - accuracy: 0.7087 - loss: 0.5705 - val_accuracy: 0.4798 - val_loss: 0.7557 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 194ms/step - accuracy: 0.7196 - loss: 0.5465 - val_accuracy: 0.6364 - val_loss: 0.6942 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 191ms/step - accuracy: 0.7249 - loss: 0.5500 - val_accuracy: 0.6953 - val_loss: 0.6660 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - accuracy: 0.7296 - loss: 0.5470 - val_accuracy: 0.7492 - val_loss: 0.6203 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 190ms/step - accuracy: 0.7425 - loss: 0.5216 - val_accuracy: 0.7593 - val_loss: 0.5949 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - accuracy: 0.7418 - loss: 0.5265 - val_accuracy: 0.7576 - val_loss: 0.6072 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 159ms/step - accuracy: 0.7489 - loss: 0.5190 - val_accuracy: 0.8468 - val_loss: 0.4983 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.7413 - loss: 0.5198 - val_accuracy: 0.7997 - val_loss: 0.5435 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.7620 - loss: 0.5054 - val_accuracy: 0.7912 - val_loss: 0.5612 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 193ms/step - accuracy: 0.7589 - loss: 0.5136 - val_accuracy: 0.7727 - val_loss: 0.5809 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - accuracy: 0.7423 - loss: 0.5253 - val_accuracy: 0.8266 - val_loss: 0.5149 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 162ms/step - accuracy: 0.7418 - loss: 0.5196 - val_accuracy: 0.7795 - val_loss: 0.5710 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.7487 - loss: 0.5173\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 189ms/step - accuracy: 0.7489 - loss: 0.5169 - val_accuracy: 0.7963 - val_loss: 0.5463 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - accuracy: 0.7688 - loss: 0.4896 - val_accuracy: 0.7896 - val_loss: 0.5551 - learning_rate: 2.5000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.7526 - loss: 0.5135 - val_accuracy: 0.8030 - val_loss: 0.5359 - learning_rate: 2.5000e-05\n",
            "Epoch 29: early stopping\n",
            "Restoring model weights from the end of the best epoch: 24.\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step\n",
            "\n",
            " Test Set Results:\n",
            "Accuracy: 0.7450\n",
            "Sensitivity (SN): 0.7100\n",
            "Specificity (SP): 0.7800\n",
            "MCC: 0.4912\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, MaxPooling1D, BatchNormalization, Dropout,\n",
        "    Bidirectional, LSTM, Dense, Flatten, Layer\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
        "                                 initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
        "        a = tf.keras.backend.softmax(e, axis=1)\n",
        "        output = tf.keras.backend.sum(x * a, axis=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "X_train = np.expand_dims(np.array(X_train), axis=2)\n",
        "X_test = np.expand_dims(np.array(X_test), axis=2)\n",
        "y_train = np.array(labels)\n",
        "y_test = np.array(new_labels)\n",
        "def build_cnn_lstm_attention(input_shape, lstm_units=32, dense_units=128,\n",
        "                             dropout_rate=0.3, lr=1e-4):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv1D(128, 5, activation='relu', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Conv1D(32, 3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Bidirectional(LSTM(lstm_units, return_sequences=True,\n",
        "                           dropout=0.3, recurrent_dropout=0.2))(x)\n",
        "    x = Attention()(x)\n",
        "\n",
        "    x = Dense(dense_units, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_cnn_lstm_attention(\n",
        "    input_shape=(X_train.shape[1], 1),\n",
        "    lstm_units=32,\n",
        "    dense_units=128,\n",
        "    dropout_rate=0.3,\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='accuracy', patience=5, restore_best_weights=True, verbose=1\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='accuracy', factor=0.5, patience=3, verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "y_pred_test = (model.predict(X_test) > 0.5).astype(int)\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "ACC = accuracy_score(y_test, y_pred_test)\n",
        "SN = recall_score(y_test, y_pred_test)\n",
        "SP = TN / (TN + FP)\n",
        "MCC = matthews_corrcoef(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\n Test Set Results:\")\n",
        "print(f\"Accuracy: {ACC:.4f}\")\n",
        "print(f\"Sensitivity (SN): {SN:.4f}\")\n",
        "print(f\"Specificity (SP): {SP:.4f}\")\n",
        "print(f\"MCC: {MCC:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZrdyDyNxW5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}